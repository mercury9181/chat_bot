FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.23249289650711379984e-01) (1, -2.87442930679605690436e-01) (2, -2.37925038378523079352e-01) (3, -2.47695007067011141455e-01) (4, -2.37069592516706673102e-01) (5, 1.40889068564088915103e+00) (6, 1.70129530924834448946e+00) (7, 1.43224804719598863834e+00) (8, 1.49973796924265001529e+00) (9, 1.40806824585111711734e+00) (10, 1.82982287856825243111e-01) (0, -7.78502755377137534154e-01) (1, 5.66473769218640010870e-01) (2, 5.28437005550579708135e-01) (3, 5.76636569769100826299e-01) (4, 4.69016159147934597051e-01) (5, 4.77166801591028555407e+00) (6, 4.72923478384127005114e+00) (7, 4.73552532394041403307e+00) (8, 4.75786214966883047595e+00) (9, 4.74378821153749807848e+00) (10, -2.38875094660528308754e-01) (0, -6.71312695998939834574e-01) (1, -1.81782358031557289557e-01) (2, -1.73777767162607399420e-01) (3, -3.16210225861356941657e-01) (4, -2.04294019084261146979e-01) (5, 1.47971739491136644595e+00) (6, 1.75868995535649497164e+00) (7, 1.39988583197506044620e+00) (8, 1.73098311248697478426e+00) (9, 1.71811424243725974215e+00) (10, -9.29253223092486035917e-02) (0, -6.63260870191895945958e-01) (1, 1.01624855621427423458e+00) (2, 9.42791605728567905231e-01) (3, 9.41215398150385684772e-01) (4, 1.03947684391111261348e+00) (5, 4.77002634008039816393e+00) (6, 4.63594005901922567858e+00) (7, 4.64253177393783911242e+00) (8, 4.73549878497232779040e+00) (9, 4.68577388573994024767e+00) (10, 1.43683219933499195520e-02) (0, -7.31277280065858348301e-01) (1, 9.69345653789938754841e-01) (2, 9.93441457289160556599e-01) (3, 9.67177944618166751667e-01) (4, 9.23219377535284824177e-01) (5, 4.65515507657636984362e+00) (6, 4.65122839708437307848e+00) (7, 4.59319589187015875353e+00) (8, 4.64839278061498983874e+00) (9, 4.66258928824772223010e+00) (10, 1.58376403399704851482e-01) (0, -1.65334952343495822369e-01) (1, 5.12618081838802974737e-01) (2, 5.23443179399685543096e-01) (3, 4.14008753568844589665e-01) (4, 3.93240997762398514226e-01) (5, 4.60061196137775763049e+00) (6, 4.65077046949972494616e+00) (7, 4.61713383991827353015e+00) (8, 4.75657682020773275866e+00) (9, 4.61584615607370718493e+00) (10, 1.56448923298119463299e-01) (0, -1.64477871079238391339e-01) (1, 4.99404246182160060918e-01) (2, 5.41390227348522823370e-01) (3, 4.36376387448029312566e-01) (4, 5.59983240635113399541e-01) (5, 4.59240656246055944933e+00) (6, 4.58035973359455450549e+00) (7, 4.65424168754925116076e+00) (8, 4.63453560595025404467e+00) (9, 4.67741902817596777453e+00) (10, 1.46653868504761614178e-01) (0, -5.03869256634506546000e-01) (1, -1.54660426717088961279e-01) (2, -3.26262377839372841315e-01) (3, -2.91486304890201775031e-01) (4, -2.78793082278059212165e-01) (5, 1.71972412291444975985e+00) (6, 1.80969708460845191134e+00) (7, 1.46508245786340807193e+00) (8, 1.70586124110497672213e+00) (9, 1.85903566378630835665e+00) (10, -1.69267770901434577713e-01) (0, 1.81554449294778041946e-01) (1, 1.75450655317967552094e-01) (2, 2.53336670494740623383e-01) (3, 3.56961558259194511322e-01) (4, 2.50860067702000755219e-01) (5, -1.03493658922674169887e+00) (6, -9.91868724153060821180e-01) (7, -1.19349435024003636130e+00) (8, -1.05072694741250982631e+00) (9, -1.14167305551530828822e+00) (10, -2.67153616526629278383e-01) (0, -7.71131378177249304784e-01) (1, 4.52760679841952118352e-01) (2, 5.87228523761944454229e-01) (3, 4.65766133935169857061e-01) (4, 4.57759360046105179265e-01) (5, 4.64995476503481253161e+00) (6, 4.70878630776514395251e+00) (7, 4.74366295237650259509e+00) (8, 4.62436934460510595812e+00) (9, 4.68759062041153296008e+00) (10, -2.68386695393331653481e-01) (11, 7.12536956630070772434e-01) (12, 2.88743291694834369743e-01) (13, 6.74724068007786836887e-01) (14, 2.32403044302179950797e-01) (15, 2.20373351771547931754e-01) (16, 3.11452840168192524040e-01) (17, 2.60197271306231159294e-01) (18, 4.91629606841300303977e-01) (19, 5.02025790962595808153e-02) (20, 2.19919223983004230583e-01) (21, 2.04803955454542774284e-01) 
